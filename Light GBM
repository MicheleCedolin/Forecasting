import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd
import time
import lightgbm as lgb

# Assuming 'NN5' DataFrame and 'features' DataFrame are already defined
Results = []
MAE = []
MSE = []
MAPE = []
SMAPE = []
TEST = []
PARAMETER = []
COMPUTATIONAL_TIME = []

def smape(y_test, y_pred):
    return np.mean((2 * np.abs(y_pred - y_test)) / (np.abs(y_test) + np.abs(y_pred)))

def mape(y_test, y_pred):
    return np.mean(np.abs(y_test - y_pred) / np.abs(y_test))

for i in range(1, 112):
    features['lag1'] = NN5.iloc[:, i].shift(1)
    features['lag2'] = NN5.iloc[:, i].shift(2)
    features['lag3'] = NN5.iloc[:, i].shift(3)
    features['lag4'] = NN5.iloc[:, i].shift(4)
    features['lag5'] = NN5.iloc[:, i].shift(5)
    features['lag6'] = NN5.iloc[:, i].shift(6)
    features['lag7'] = NN5.iloc[:, i].shift(7)
    features['Roll_mean'] = NN5.iloc[:, i].rolling(window=7).mean()

    # Drop rows with NaN values
    features = features.dropna()

    train_data = features.iloc[:int(features.shape[0] * 0.93)]
    test_data = features.iloc[int(features.shape[0] * 0.93):]

    x_train = np.array(train_data[['weekday', 'lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'Roll_mean']])
    x_test = np.array(test_data[['weekday', 'lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'Roll_mean']])
    y_train = np.array(train_data.iloc[:, i])
    y_test = np.array(test_data.iloc[:, i])

    # Fit scaler on training data and transform both training and test data
    scaler = MinMaxScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)

    # LightGBM hyperparameters grid
      lgb_params = {
          'num_leaves': [31, 50],
          'learning_rate': [0.01, 0.05, 0.1],
          'n_estimators': [20, 40, 100]
      }
    lgbm = lgb.LGBMRegressor()
    lgb_search = GridSearchCV(lgbm, lgb_params, cv=3, verbose=3)

    start_time = time.time()  # Start time before training
    lgb_search.fit(x_train_scaled, y_train.ravel())
    lgb_confirmed = lgb_search.best_estimator_
    lgb_parameters = np.array(lgb_confirmed)
    y_lgb_pred = lgb_confirmed.predict(x_test_scaled)
    end_time = time.time()  # End time after training

    computational_time = end_time - start_time
    COMPUTATIONAL_TIME.append(computational_time)

    mae_lgb = mean_absolute_error(y_test, y_lgb_pred)
    mse_lgb = mean_squared_error(y_test, y_lgb_pred)

    MAE.append(mae_lgb)
    MSE.append(mse_lgb)
    MAPE.append(mape(y_test, y_lgb_pred))
    SMAPE.append(smape(y_test, y_lgb_pred))

    Results.append(y_lgb_pred)
    TEST.append(y_test)
    PARAMETER.append(lgb_parameters)

Parameter = pd.DataFrame(PARAMETER)

error = [MAE, MSE, MAPE, SMAPE]
Error = pd.DataFrame(error, index=["MAE", "MSE", "MAPE", "SMAPE"]).T  # Use index to label rows instead of columns
Forecast = pd.DataFrame(Results).T
ComputationalTime = pd.DataFrame(COMPUTATIONAL_TIME, columns=["Computational Time"])

Error.to_csv('/content/drive/My Drive/NN5Results/LGBError.csv', index=False)
Forecast.to_csv('/content/drive/My Drive/NN5Results/LGBForecast.csv', index=False)
Parameter.to_csv('/content/drive/My Drive/NN5Results/LGBBestParameters.csv', index=False)
ComputationalTime.to_csv('/content/drive/My Drive/NN5Results/LGBComputationalTime.csv', index=False)
